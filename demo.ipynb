{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get scraped.parquet from S3 and write it to Kafka using the REST Proxy.\n",
    "from kafi.kafi import *\n",
    "s = S3(\"local\")\n",
    "r = RestProxy(\"local\")\n",
    "r.rm(\"scraped_json\")\n",
    "r.from_file(s, \"scraped.parquet\", \"scraped_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: 1000\n",
      "Read: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Kafka topic and carbon-copy it to local disk.\n",
    "from kafi.kafi import *\n",
    "c = Cluster(\"local\")\n",
    "l = Local(\"local\")\n",
    "l.rm(\"scraped_json\")\n",
    "c.cp(\"scraped_json\", l, \"scraped_json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"name\": \"twitter\" with \"name\": \"x\" on local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: 1000\n",
      "Read: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write back the changed topic to Kafka.\n",
    "from kafi.kafi import *\n",
    "l = Local(\"local\")\n",
    "c = Cluster(\"local\")\n",
    "c.retouch(\"scraped_json\")\n",
    "l.cp(\"scraped_json\", c, \"scraped_json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'topic': 'scraped_json',\n",
       "   'headers': None,\n",
       "   'partition': 0,\n",
       "   'offset': 0,\n",
       "   'timestamp': (1, 1695547359286),\n",
       "   'key': None,\n",
       "   'value': {'datetime': '2022-11-28 10:49:02+00:00',\n",
       "    'text': 'ğŸ’¨çˆ†é€Ÿãƒ¬ã‚¹å–¶æ¥­ä¸­ğŸ’¨\\nãƒ•ã‚©ãƒ­ãƒ¼&amp;ğŸˆ¯ï¸ï¼500ğŸˆ¹\\n\\nâœ¨Acapulco Goldâœ¨ã€”äººæ°—ã€•\\nğŸ­Rainbow CandyğŸ­\\n 1å€‹6000 3å€‹5500\\n\\nã€æ¥µå¯†ã€‘\\nğŸ¯Hulk BerryğŸ¯1.2ãœ\\n1/12000 3/11000\\n\\nè³ªãƒ»åŒ‚ã„ğŸ’¯\\né…é”â­•ï¸å³å¯¾å¿œâ­•\\nï¸ğŸˆ¯ï¸â†’å ºğŸ“\\nğŸ“²ãƒ†ãƒ¬â†’ https://t.co/cM4OM8u7jo\\n\\nå¤§é˜ªæ‰‹æŠ¼ã— å ºæ‰‹æŠ¼ã—\\nãƒŸãƒŠãƒŸæ‰‹æŠ¼ã—  é‡èœæ‰‹æŠ¼ã—',\n",
       "    'source': {'id': '1597180760364298241',\n",
       "     'name': 'x',\n",
       "     'user': 'Kansaiwa2525'}}}],\n",
       " 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the first message\n",
    "from kafi.kafi import *\n",
    "c = Cluster(\"local\")\n",
    "c.head(\"scraped_json\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ralph/kafka/kafi/venv/lib/python3.10/site-packages/confluent_kafka/schema_registry/protobuf.py:513: UserWarning: MessageFactory class is deprecated. Please use GetMessageClass() instead of MessageFactory.GetPrototype. MessageFactory class will be removed after 2024.\n",
      "  self._msg_class = MessageFactory().GetPrototype(descriptor)\n",
      "W0000 00:00:1695547446.115999   57335 parser.cc:676] No syntax specified for the proto file: schema_2.proto. Please use 'syntax = \"proto2\";' or 'syntax = \"proto3\";' to specify a syntax version. (Defaulted to proto2 syntax.)\n"
     ]
    },
    {
     "ename": "ResourceExistsError",
     "evalue": "The specified blob already exists.\nRequestId:f8a70d69-00a2-4f82-b973-e4102384b6d5\nTime:2023-09-24T09:24:07.178Z\nErrorCode:BlobAlreadyExists\nContent: <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<Error>\n  <Code>BlobAlreadyExists</Code>\n  <Message>The specified blob already exists.\nRequestId:f8a70d69-00a2-4f82-b973-e4102384b6d5\nTime:2023-09-24T09:24:07.178Z</Message>\n</Error>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExistsError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/ralph/kafka/kafi/demo.ipynb Zelle 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ralph/kafka/kafi/demo.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m c \u001b[39m=\u001b[39m Cluster(\u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ralph/kafka/kafi/demo.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m a \u001b[39m=\u001b[39m AzureBlob(\u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ralph/kafka/kafi/demo.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m c\u001b[39m.\u001b[39;49mto_file(\u001b[39m\"\u001b[39;49m\u001b[39mscored_protobuf\u001b[39;49m\u001b[39m\"\u001b[39;49m, a, \u001b[39m\"\u001b[39;49m\u001b[39mscored.xlsx\u001b[39;49m\u001b[39m\"\u001b[39;49m, value_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprotobuf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/kafka/kafi/kafi/files.py:42\u001b[0m, in \u001b[0;36mFiles.to_file\u001b[0;34m(self, topic, fs_obj, file, n, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m data_bytes \u001b[39m=\u001b[39m data_bytesIO\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m     41\u001b[0m abs_path_file_str \u001b[39m=\u001b[39m fs_obj\u001b[39m.\u001b[39madmin\u001b[39m.\u001b[39mget_abs_path_str(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfiles/\u001b[39m\u001b[39m{\u001b[39;00mfile_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m fs_obj\u001b[39m.\u001b[39;49madmin\u001b[39m.\u001b[39;49mwrite_bytes(abs_path_file_str, data_bytes)\n\u001b[1;32m     43\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(df)\n",
      "File \u001b[0;32m~/kafka/kafi/kafi/fs/azureblob/azureblob_admin.py:96\u001b[0m, in \u001b[0;36mAzureBlobAdmin.write_bytes\u001b[0;34m(self, abs_path_file_str, data_bytes)\u001b[0m\n\u001b[1;32m     94\u001b[0m blobClient \u001b[39m=\u001b[39m BlobClient\u001b[39m.\u001b[39mfrom_connection_string(conn_str\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstorage_obj\u001b[39m.\u001b[39mazure_blob_config_dict[\u001b[39m\"\u001b[39m\u001b[39mconnection.string\u001b[39m\u001b[39m\"\u001b[39m], container_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstorage_obj\u001b[39m.\u001b[39mazure_blob_config_dict[\u001b[39m\"\u001b[39m\u001b[39mcontainer.name\u001b[39m\u001b[39m\"\u001b[39m], blob_name\u001b[39m=\u001b[39mabs_path_file_str)\n\u001b[1;32m     95\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m blobClient\u001b[39m.\u001b[39;49mupload_blob(data_bytes)\n",
      "File \u001b[0;32m~/kafka/kafi/venv/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/kafka/kafi/venv/lib/python3.10/site-packages/azure/storage/blob/_blob_client.py:756\u001b[0m, in \u001b[0;36mBlobClient.upload_blob\u001b[0;34m(self, data, blob_type, length, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upload_blob_options(\n\u001b[1;32m    750\u001b[0m     data,\n\u001b[1;32m    751\u001b[0m     blob_type\u001b[39m=\u001b[39mblob_type,\n\u001b[1;32m    752\u001b[0m     length\u001b[39m=\u001b[39mlength,\n\u001b[1;32m    753\u001b[0m     metadata\u001b[39m=\u001b[39mmetadata,\n\u001b[1;32m    754\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    755\u001b[0m \u001b[39mif\u001b[39;00m blob_type \u001b[39m==\u001b[39m BlobType\u001b[39m.\u001b[39mBlockBlob:\n\u001b[0;32m--> 756\u001b[0m     \u001b[39mreturn\u001b[39;00m upload_block_blob(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    757\u001b[0m \u001b[39mif\u001b[39;00m blob_type \u001b[39m==\u001b[39m BlobType\u001b[39m.\u001b[39mPageBlob:\n\u001b[1;32m    758\u001b[0m     \u001b[39mreturn\u001b[39;00m upload_page_blob(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/kafka/kafi/venv/lib/python3.10/site-packages/azure/storage/blob/_upload_helpers.py:195\u001b[0m, in \u001b[0;36mupload_block_blob\u001b[0;34m(client, data, stream, length, overwrite, headers, validate_content, max_concurrency, blob_settings, encryption_options, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mexcept\u001b[39;00m HttpResponseError \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    194\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m         process_storage_error(error)\n\u001b[1;32m    196\u001b[0m     \u001b[39mexcept\u001b[39;00m ResourceModifiedError \u001b[39mas\u001b[39;00m mod_error:\n\u001b[1;32m    197\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m overwrite:\n",
      "File \u001b[0;32m~/kafka/kafi/venv/lib/python3.10/site-packages/azure/storage/blob/_shared/response_handlers.py:184\u001b[0m, in \u001b[0;36mprocess_storage_error\u001b[0;34m(storage_error)\u001b[0m\n\u001b[1;32m    181\u001b[0m error\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m (error\u001b[39m.\u001b[39mmessage,)\n\u001b[1;32m    182\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[39m# `from None` prevents us from double printing the exception (suppresses generated layer error context)\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     exec(\u001b[39m\"\u001b[39;49m\u001b[39mraise error from None\u001b[39;49m\u001b[39m\"\u001b[39;49m)   \u001b[39m# pylint: disable=exec-used # nosec\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mSyntaxError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    186\u001b[0m     \u001b[39mraise\u001b[39;00m error \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "File \u001b[0;32m~/kafka/kafi/venv/lib/python3.10/site-packages/azure/storage/blob/_upload_helpers.py:105\u001b[0m, in \u001b[0;36mupload_block_blob\u001b[0;34m(client, data, stream, length, overwrite, headers, validate_content, max_concurrency, blob_settings, encryption_options, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     encryption_data, data \u001b[39m=\u001b[39m encrypt_blob(data, encryption_options[\u001b[39m'\u001b[39m\u001b[39mkey\u001b[39m\u001b[39m'\u001b[39m], encryption_options[\u001b[39m'\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    103\u001b[0m     headers[\u001b[39m'\u001b[39m\u001b[39mx-ms-meta-encryptiondata\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m encryption_data\n\u001b[0;32m--> 105\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mupload(\n\u001b[1;32m    106\u001b[0m     body\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    107\u001b[0m     content_length\u001b[39m=\u001b[39;49madjusted_count,\n\u001b[1;32m    108\u001b[0m     blob_http_headers\u001b[39m=\u001b[39;49mblob_headers,\n\u001b[1;32m    109\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    110\u001b[0m     \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49mreturn_response_headers,\n\u001b[1;32m    111\u001b[0m     validate_content\u001b[39m=\u001b[39;49mvalidate_content,\n\u001b[1;32m    112\u001b[0m     data_stream_total\u001b[39m=\u001b[39;49madjusted_count,\n\u001b[1;32m    113\u001b[0m     upload_stream_current\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    114\u001b[0m     tier\u001b[39m=\u001b[39;49mtier\u001b[39m.\u001b[39;49mvalue \u001b[39mif\u001b[39;49;00m tier \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    115\u001b[0m     blob_tags_string\u001b[39m=\u001b[39;49mblob_tags_string,\n\u001b[1;32m    116\u001b[0m     immutability_policy_expiry\u001b[39m=\u001b[39;49mimmutability_policy_expiry,\n\u001b[1;32m    117\u001b[0m     immutability_policy_mode\u001b[39m=\u001b[39;49mimmutability_policy_mode,\n\u001b[1;32m    118\u001b[0m     legal_hold\u001b[39m=\u001b[39;49mlegal_hold,\n\u001b[1;32m    119\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m progress_hook:\n\u001b[1;32m    122\u001b[0m     progress_hook(adjusted_count, adjusted_count)\n",
      "File \u001b[0;32m~/kafka/kafi/venv/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/kafka/kafi/venv/lib/python3.10/site-packages/azure/storage/blob/_generated/operations/_block_blob_operations.py:864\u001b[0m, in \u001b[0;36mBlockBlobOperations.upload\u001b[0;34m(self, content_length, body, timeout, transactional_content_md5, metadata, tier, request_id_parameter, blob_tags_string, immutability_policy_expiry, immutability_policy_mode, legal_hold, transactional_content_crc64, blob_http_headers, lease_access_conditions, cpk_info, cpk_scope_info, modified_access_conditions, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m response \u001b[39m=\u001b[39m pipeline_response\u001b[39m.\u001b[39mhttp_response\n\u001b[1;32m    863\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m201\u001b[39m]:\n\u001b[0;32m--> 864\u001b[0m     map_error(status_code\u001b[39m=\u001b[39;49mresponse\u001b[39m.\u001b[39;49mstatus_code, response\u001b[39m=\u001b[39;49mresponse, error_map\u001b[39m=\u001b[39;49merror_map)\n\u001b[1;32m    865\u001b[0m     error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize\u001b[39m.\u001b[39mfailsafe_deserialize(_models\u001b[39m.\u001b[39mStorageError, pipeline_response)\n\u001b[1;32m    866\u001b[0m     \u001b[39mraise\u001b[39;00m HttpResponseError(response\u001b[39m=\u001b[39mresponse, model\u001b[39m=\u001b[39merror)\n",
      "File \u001b[0;32m~/kafka/kafi/venv/lib/python3.10/site-packages/azure/core/exceptions.py:112\u001b[0m, in \u001b[0;36mmap_error\u001b[0;34m(status_code, response, error_map)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    111\u001b[0m error \u001b[39m=\u001b[39m error_type(response\u001b[39m=\u001b[39mresponse)\n\u001b[0;32m--> 112\u001b[0m \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[0;31mResourceExistsError\u001b[0m: The specified blob already exists.\nRequestId:f8a70d69-00a2-4f82-b973-e4102384b6d5\nTime:2023-09-24T09:24:07.178Z\nErrorCode:BlobAlreadyExists\nContent: <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<Error>\n  <Code>BlobAlreadyExists</Code>\n  <Message>The specified blob already exists.\nRequestId:f8a70d69-00a2-4f82-b973-e4102384b6d5\nTime:2023-09-24T09:24:07.178Z</Message>\n</Error>"
     ]
    }
   ],
   "source": [
    "# Write the scored topic to Azure Blob Storage (as Excel)\n",
    "from kafi.kafi import *\n",
    "c = Cluster(\"local\")\n",
    "a = AzureBlob(\"local\")\n",
    "c.to_file(\"scored_protobuf\", a, \"scored.xlsx\", value_type=\"protobuf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
